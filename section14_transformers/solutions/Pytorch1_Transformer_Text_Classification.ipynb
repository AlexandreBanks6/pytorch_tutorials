{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Classifying text with Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We'll be using Pytorch's text library called torchtext! \n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 50\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_len = 128\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "dataset_test = AG_NEWS(root=data_set_root, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label, text = next(iter(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer is the method by which we split the sentence into \"chunks\" or \"tokens\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# The vocab is all the unique tokens contained within our dataset\n",
    "# and provides each token with it's own integer index.\n",
    "\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(dataset_train),\n",
    "    min_freq=2, # Only include a token if it appears more than 2 times in the dataset\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'], # special case tokens\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set the <unk> \"unknown\" token as the default token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the vocab!\n",
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cab define \n",
    "text_tranform = T.Sequential(\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = lambda batch: [tokenizer(x) for x in batch]\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "\n",
    "# Transformer block with self-attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = self.multihead_attn(x, x, x)[0] + x\n",
    "        \n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x) + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# \"Encoder-Only\" Style Transformer\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128):\n",
    "        super(NanoTransformer, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.mlp_in = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                     nn.LayerNorm(hidden_size),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=1, batch_first=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        bs, l = input_seq.shape\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        \n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(bs, l, -1)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        emb = self.mlp_in(embs)\n",
    "        \n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs)\n",
    "        \n",
    "        return self.fc_out(output), attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "# Create model\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=4, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, nepochs, leave=False, desc=\"Epoch\")    \n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Test %.2f%%' % (train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    tf_classifier.train()\n",
    "    steps = 0\n",
    "    for label, text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        bs = label.shape[0]\n",
    "        text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "        text_tokens = td(text_tokens)\n",
    "        label = (label - 1).to(device)\n",
    "\n",
    "        pred, attn_map = tf_classifier(text_tokens)\n",
    "\n",
    "        loss = loss_fn(pred[:, -1, :], label)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        train_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "        steps += bs\n",
    "        \n",
    "    train_acc = (train_acc/steps).item()\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    tf_classifier.eval()\n",
    "    steps = 0\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(data_loader_test, desc=\"Testing\", leave=False):\n",
    "            bs = label.shape[0]\n",
    "            text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "            label = (label - 1).to(device)\n",
    "\n",
    "            pred, attn_map = tf_classifier(text_tokens)\n",
    "\n",
    "            loss = loss_fn(pred[:, -1, :], label)\n",
    "            test_loss_logger.append(loss.item())\n",
    "\n",
    "            test_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "            steps += bs\n",
    "\n",
    "        test_acc = (test_acc/steps).item()\n",
    "        test_acc_logger.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b79069",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "print(\"Max Test Accuracy %.2f%%\" % (np.max(test_acc_logger) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "label, text = next(iter(data_loader_test))\n",
    "tf_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "    pred, attn_map = tf_classifier(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79532439",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "text[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(attn_map[index][-1].detach().cpu().flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbe046",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = attn_map[index][-1].argsort(descending=True)[:5]\n",
    "vocab.lookup_tokens(text_tokens[index, top_5].cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
