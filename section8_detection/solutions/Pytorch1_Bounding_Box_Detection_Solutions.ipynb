{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sb01NHS5PMS8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.models as models\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from Trainer import ModelTrainer\n",
    "from Datasets import CUB200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch version >= 0.15 have v2 transforms that work on both the images and the bounding boxes\n",
    "# https://pytorch.org/blog/extending-torchvisions-transforms-to-object-detection-segmentation-and-video-tasks/\n",
    "# It's currently in beta so you'll get a warning if you try to use it!\n",
    "\n",
    "# import torchvision.transforms.v2 as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUaeH517PMS_"
   },
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 64\n",
    "\n",
    "# How many itterations of our dataset\n",
    "num_epochs = 30\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Where to load/save the dataset from \n",
    "# data_set_root = \"../../datasets\"\n",
    "data_set_root = \"/media/luke/Quick Storage/Data/cub_200\"\n",
    "\n",
    "# What to resize our images to \n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVGVcxx0PMTB"
   },
   "outputs": [],
   "source": [
    "start_from_checkpoint = True\n",
    "\n",
    "save_dir = '../data/Models'\n",
    "model_name = 'ResNet18_CUB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRJXAwTXPMTD"
   },
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wDY0BijPMTF"
   },
   "source": [
    "# Data Augmentation Transform\n",
    "After training ResNet with no augmentations record the results, then implement the data augmentation. <br>\n",
    "With a small dataset our large model will more then likely simply overfit to (or memorize) the training data which will often lead to bad evaluation results<br>\n",
    "We can \"create more\" data from our limited dataset by applying random transformations as we sample images from our dataset instead of simply resizing them<br>\n",
    "By applying these transformations we are also forcing our model to generalise better to unseen images<br>\n",
    "You can also apply random affine transformations (shifts, scaling, rotations etc) - see <a href=\"https://pytorch.org/vision/0.12/\">Pytorch documentations</a>.<br>\n",
    "NOTE: you should only apply transforms that make sense, eg if at test time you'll never see an upside-down cat, don't flip your images vertically \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keIwAFK-PMTG"
   },
   "outputs": [],
   "source": [
    "# Only include the augmentations if you can use the v2 transforms that will augment \n",
    "# both the image and bounding boxes (you'll need to modify the dataset class too!)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.ToTensor(),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.RandomRotation(10),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225]),\n",
    "                                     ])\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BboxIOU(nn.Module):\n",
    "    \n",
    "    def xyhw_to_xyxy(self, bbox):\n",
    "        bbox[:, 2] += bbox[:, 0]\n",
    "        bbox[:, 3] += bbox[:, 1]\n",
    "        return bbox\n",
    "\n",
    "    def bb_intersection_over_union(self, pred_bbox, target_bbox):\n",
    "        pred_bbox = self.xyhw_to_xyxy(pred_bbox)\n",
    "        target_bbox = self.xyhw_to_xyxy(target_bbox)\n",
    "\n",
    "        # determine the (x, y)-coordinates of the intersection rectangle\n",
    "        xA = torch.cat((pred_bbox[:, 0:1], target_bbox[:, 0:1]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        yA = torch.cat((pred_bbox[:, 1:2], target_bbox[:, 1:2]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        xB = torch.cat((pred_bbox[:, 2:3], target_bbox[:, 2:3]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "        yB = torch.cat((pred_bbox[:, 3:4], target_bbox[:, 3:4]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        # compute the area of intersection rectangle\n",
    "        x_area = (xB - xA).abs()\n",
    "        y_area = (yB - yA).abs()\n",
    "        interArea = x_area * y_area\n",
    "\n",
    "        w1 = (pred_bbox[:, 0:1] - pred_bbox[:, 2:3]).abs()\n",
    "        h1 = (pred_bbox[:, 1:2] - pred_bbox[:, 3:4]).abs()\n",
    "\n",
    "        w2 = (target_bbox[:, 0:1] - target_bbox[:, 2:3]).abs()\n",
    "        h2 = (target_bbox[:, 1:2] - target_bbox[:, 3:4]).abs()\n",
    "\n",
    "        area1 = w1 * h1\n",
    "        area2 = w2 * h2\n",
    "\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / (area1 + area2 - interArea)\n",
    "\n",
    "        # return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        pred_bbox = torch.sigmoid(predictions[:, :4])\n",
    "        target_bbox = data[1].to(pred_bbox.device)\n",
    "        \n",
    "        return self.bb_intersection_over_union(pred_bbox, target_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7L2lrkdPMTM"
   },
   "source": [
    "# Create the training, testing and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29503,
     "status": "ok",
     "timestamp": 1568947936500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "5FyAAqHWPMTM",
    "outputId": "d566a865-6439-47d3-a195-6b897199d923"
   },
   "outputs": [],
   "source": [
    "# Define our STL10 Datasets\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.STL10\n",
    "\n",
    "# Dataset definition is a bit differenet to MNIST and CIFAR10\n",
    "# STL10 has 3 different datasets, test, train and unlabeled\n",
    "# http://ai.stanford.edu/~acoates/stl10/\n",
    "# training set only has 5000 images and test set only 8000\n",
    "# Image size in this dataset are 96x96, larger then what we've been using\n",
    "\n",
    "train_data = CUB200(data_set_root, transform=train_transform, image_size=image_size, test_train=0)\n",
    "test_data = CUB200(data_set_root, transform=transform, image_size=image_size, test_train=1)\n",
    "\n",
    "# Split trainging data into train and validation set with 90/10% traning/validation split\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data)*validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples],\n",
    "                                                       generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGdAgnKgPMTc"
   },
   "source": [
    "# Create the Pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34657,
     "status": "ok",
     "timestamp": 1568947941813,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "JQPwhQuaPMTd",
    "outputId": "000d7dfc-5cd1-4afc-ef52-d7c1c7cc2754"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the ResNet18 Model\n",
    "res_net = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model=res_net.to(device), output_size=4, device=device, \n",
    "                             loss_fun=nn.BCEWithLogitsLoss(), batch_size=batch_size, \n",
    "                             learning_rate=learning_rate, save_dir=save_dir, model_name=model_name,\n",
    "                             elav_metric=BboxIOU(), start_from_checkpoint=start_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data, test_set=test_data, val_set=valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Learning Rate Scheduler\n",
    "We can dynamically change the <a href=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\">learning rate</a> during training to help our model converge to a better minimum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, \n",
    "                                                        step_size=1, \n",
    "                                                        gamma=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30371,
     "status": "ok",
     "timestamp": 1568947937416,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "2ET6pMrYPMTa",
    "outputId": "9131cd8c-eab1-4a66-d4fc-1314ae775814"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, bbox, labels = next(iter(model_trainer.test_loader))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indx = 0\n",
    "ex_img = images[example_indx]\n",
    "ex_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "ex_label[:, 2] += ex_label[:, 0]\n",
    "ex_label[:, 3] += ex_label[:, 1]\n",
    "\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, ex_label, colors=(0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model!\n",
    "Our full training method is now fully contained within the trainner class! Simply run the run_training method and specify how many epochs it should train for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1568948678396,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "K27MEsO5PMT-",
    "outputId": "0c03f2f2-e250-4fad-dae5-b0dbaad8bda4"
   },
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1568948455980,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HoLp_P3xPMUE",
    "outputId": "b241900f-ff45-42f0-dc33-14b48126836f"
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image to test\n",
    "example_indx = 0\n",
    "ex_img = images[example_indx]\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "\n",
    "# Get the model's prediction for the Bounding Box\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = torch.sigmoid(model_trainer(ex_img.unsqueeze(0).to(device)))\n",
    "    ex_label = (pred_out * image_size).cpu()\n",
    "    ex_label[:, 2] += ex_label[:, 0]\n",
    "    ex_label[:, 3] += ex_label[:, 1]\n",
    "    \n",
    "# Draw the box on the image\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, ex_label, colors=(0, 255, 0))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_F2Qy9WPMUG"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1568948469315,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "dKMx57tEPMUH",
    "outputId": "7590031a-2a9e-4701-9799-320155e5efd6"
   },
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18_STL10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
