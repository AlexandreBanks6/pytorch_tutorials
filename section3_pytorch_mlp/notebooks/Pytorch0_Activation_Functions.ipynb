{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Gradient Decent Revisit with Pytorch</h1> <br>\n",
    "Animation of our \"model\" at each step when training with gradient descent\n",
    "\n",
    "<b>With our new knowledge of Pytorch let's create a new implementation of gradient decent!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_points):\n",
    "    data_class1 = torch.rand(data_points, 2) * 1.45\n",
    "    print(data_class1.shape)\n",
    "\n",
    "    data_class2 = torch.rand(data_points, 2) * 3\n",
    "    print(data_class2.shape)\n",
    "    \n",
    "    mask = ~((data_class2[:, 0] < 1.55) * (data_class2[:, 1] < 1.55))\n",
    "    data_class2 = data_class2[mask]\n",
    "\n",
    "    # Lables\n",
    "    data_label1 = torch.zeros(data_class1.shape[0], 1)\n",
    "    data_label2 = torch.ones(data_class2.shape[0], 1)\n",
    "\n",
    "    # Combine data\n",
    "    x_data = torch.cat((data_class1, data_class2), 0)\n",
    "    y_data = torch.cat((data_label1, data_label2), 0)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "x_train, y_train = create_data(1000)\n",
    "x_test, y_test = create_data(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what the data looks like\n",
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=y_train.flatten().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_linear = nn.Linear(2, 1) \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.Adam(logistic_linear.parameters(), lr=0.1)\n",
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_linear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test[:, 0].numpy(), x_test[:, 1].numpy(), c=class_pred.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 1\n",
    "w1 = -(1.5 * w0)\n",
    "w_0 = torch.FloatTensor([[w0, 0], [0, w0]])\n",
    "b_0 = torch.FloatTensor([[w1, w1]])\n",
    "print(\"Weights\", w_0.numpy())\n",
    "print(\"bias\", b_0.numpy())\n",
    "\n",
    "h1 = F.linear(x_train, w_0, bias=b_0)\n",
    "h2 = torch.sigmoid(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 5))\n",
    "plt.subplot(131)\n",
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Raw Data\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(h1[:, 0].numpy(), h1[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Raw output\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(h2[:, 0].numpy(), h2[:, 1].numpy(), c=y_train.flatten().numpy())\n",
    "plt.title(\"Sigmoid output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point0 = torch.tensor([0.6, 0])\n",
    "point1 = torch.tensor([0, 0.6])\n",
    "w0 = 1\n",
    "w1 = 1\n",
    "w2 = -0.6\n",
    "w_1 = torch.FloatTensor([[w0, w1]])\n",
    "b_1 = torch.FloatTensor([w2])\n",
    "print(\"Weights\", w_1.numpy())\n",
    "print(\"bias\", b_1.numpy())\n",
    "\n",
    "h3 = F.linear(h2, w_1, bias=b_1)\n",
    "class_pred = (h3 > 0).float()\n",
    "\n",
    "acc = float(sum(class_pred == y_train))/ float(y_train.shape[0])\n",
    "print(\"Non-linear accuracy %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:, 0].numpy(), x_train[:, 1].numpy(), c=class_pred.flatten().numpy())\n",
    "plt.title(\"Raw Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train a Non-Linear Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(NonLinearModel, self).__init__() \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)         \n",
    "        self.linear2 = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h1 = self.linear1(x)\n",
    "        self.h2 = torch.sigmoid(self.h1)\n",
    "        self.h3 = self.linear2(self.h2)\n",
    "        \n",
    "        return self.h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_nonlinear = NonLinearModel(input_size=2, output_size=1, hidden_size=2) \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.Adam(logistic_nonlinear.parameters(), lr=1e-2)\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 5000\n",
    "\n",
    "for epoch in trange(max_epoch, desc=\"Training Epochs\"):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_nonlinear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_nonlinear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_loss_log[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_hat = logistic_nonlinear(x_test)\n",
    "    class_pred = (y_test_hat > 0).float()\n",
    "    \n",
    "plt.figure(figsize=(5, 5))    \n",
    "plt.scatter(x_test[:, 0].numpy(), x_test[:, 1].numpy(), c=class_pred.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(logistic_nonlinear.h2[:, 0].numpy(), logistic_nonlinear.h2[:, 1].numpy(), c=class_pred.flatten().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
