{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7483c6",
   "metadata": {},
   "source": [
    "# Attention Basics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8491bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d26dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd054b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "dataset = datasets.MNIST(data_set_root, train=True, download=True, transform=transform)\n",
    "\n",
    "num_of_examples = 100\n",
    "rand_perm = torch.randperm(dataset.data.shape[0])[:num_of_examples]\n",
    "dataset_tensor = torch.cat([dataset.__getitem__(i)[0].reshape(1, -1) for i in rand_perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the images\n",
    "out = torchvision.utils.make_grid(dataset_tensor.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca862285",
   "metadata": {},
   "source": [
    "## Indexing a Dataset\n",
    "At this point we would be familiar with indexing a tensor/array with an integer index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our index value\n",
    "q_index = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the image at this index!\n",
    "plt.figure(figsize = (5,5))\n",
    "_ = plt.imshow(dataset_tensor[q_index].reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864443a",
   "metadata": {},
   "source": [
    "## Indexing a Dataset with Matrix Multiplication\n",
    "Did you know we can do the same thing, but using Matrix multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our index into a ont-hot-coded vector the sample length as the number of samples\n",
    "# we will call this our \"query\" vector (q)\n",
    "q_one_hot_vec = F.one_hot(torch.tensor([q_index]), num_of_examples)\n",
    "\n",
    "# Create a unique one-hot-coded vector for every image in our set\n",
    "# we will call this our \"key\" vector (k)\n",
    "k_one_hot = F.one_hot(torch.arange(num_of_examples), num_of_examples)\n",
    "\n",
    "# Randomly shuffle the keys and dataset to show that we will find the target image\n",
    "# even in a randomly organised dataset\n",
    "rand_perm = torch.randperm(num_of_examples)\n",
    "k_one_hot = k_one_hot[rand_perm]\n",
    "dataset_tensor_random = dataset_tensor[rand_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24751fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply our key vector \n",
    "index_map = torch.mm(q_one_hot_vec, k_one_hot.t()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.mm(index_map, dataset_tensor_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the image at this index!\n",
    "plt.figure(figsize = (5,5))\n",
    "_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e23c",
   "metadata": {},
   "source": [
    "## Attention as a \"Soft\" Look-up\n",
    "What if we don't use \"hard\" one-hot coded vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 512\n",
    "\n",
    "# Create a random query vector\n",
    "q_random_vec = torch.randn(1, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" \n",
    "attention_map = torch.mm(q_random_vec, random_keys.t()).float()\n",
    "\n",
    "# Calculate the Softmax over the all over the attention map\n",
    "attention_map = F.softmax(attention_map, 1)\n",
    "\n",
    "# Use the attention map to soft \"index\" over the dataste\n",
    "output = torch.mm(attention_map, dataset_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The largest Softmax value is %f\" % attention_map.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the image we get as a result!\n",
    "plt.figure(figsize = (5,5))\n",
    "_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a9d7e",
   "metadata": {},
   "source": [
    "## Multiple Queries\n",
    "We can also perform multiple queries at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Queries!\n",
    "num_q = 8\n",
    "\n",
    "# Create random query vectors\n",
    "q_random_vec = torch.randn(num_q, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" \n",
    "attention_map = torch.mm(q_random_vec, random_keys.transpose(0, 1)).float()\n",
    "\n",
    "# Calculate the Softmax over the all over the attention map\n",
    "attention_map = F.softmax(attention_map, -1)\n",
    "\n",
    "# Use the attention map to soft \"index\" over the dataste\n",
    "output = torch.mm(attention_map, dataset_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af670e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(output.reshape(num_q, 1, 28, 28), 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fca92",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention\n",
    "We can also perform Attention multiple times in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Queries!\n",
    "num_q = 8\n",
    "\n",
    "# Number of Heads!\n",
    "num_heads = 4\n",
    "\n",
    "# Create random query vectors\n",
    "q_random_vec = torch.randn(num_heads, num_q, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_heads, num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" \n",
    "attention_map = torch.bmm(q_random_vec, random_keys.transpose(1, 2)).float()\n",
    "\n",
    "# Calculate the Softmax over the all over the attention map\n",
    "attention_map = F.softmax(attention_map, 2)\n",
    "\n",
    "# Use the attention map to soft \"index\" over the dataste\n",
    "output = torch.bmm(attention_map, dataset_tensor.unsqueeze(0).expand(num_heads, num_of_examples, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_reshape = output.reshape(num_heads, num_q, 28, 28).transpose(1, 2).reshape(num_heads, 1, 28, num_q*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab43da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(out_reshape, 1, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718c007",
   "metadata": {},
   "source": [
    "## Pytorch Multi-Head Attention\n",
    "Of course Pytorch has it's own implementation of Multi-Head Attention!<br>\n",
    "\n",
    "[Pytorch MultiheadAttention](https://pytorch.org/docs/2.1/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)\n",
    "\n",
    "We'll go into more detail on how to use it in later examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Heads!\n",
    "num_heads = 8\n",
    "\n",
    "# Batch Size!\n",
    "batch_size = 32\n",
    "\n",
    "# Create a batch of a single random query vector\n",
    "query = torch.randn(batch_size, 1, num_heads * vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "key = torch.randn(batch_size, num_of_examples, num_heads * vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "value = torch.randn(batch_size, num_of_examples, num_heads * vec_size)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(num_heads * vec_size, num_heads, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a forward pass through the Multi-Head Attention\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value, average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass returns two things\n",
    "\n",
    "# The softmaxed \"attention mask\"\n",
    "print(\"Softmax Attention Mask\", attn_output_weights.shape)\n",
    "\n",
    "# The output of the attention block\n",
    "print(\"Attention Output\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each query/key/value vector is passed through a \"projection\" aka a learnable linear layer before\n",
    "# the attention mechanism \n",
    "# As they are all the same size Pytorch creates a single block of parameters splits it into 3 \n",
    "# Before doing a forward pass of each*\n",
    "print(\"Projection weight size\", multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "# *Most of the time, doing a deep dive into the implementation Pytorch tries to do a lot of optimisation\n",
    "# to try and be efficient as possible depending on the use-case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9bb9b",
   "metadata": {},
   "source": [
    "## Train a Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTest(nn.Module):\n",
    "    def __init__(self, num_of_examples=100, embed_dim=784, num_heads=4):\n",
    "        super(AttentionTest, self).__init__()\n",
    "        \n",
    "        self.img_mlp = nn.Sequential(nn.Linear(784, embed_dim), \n",
    "                                     nn.LayerNorm(embed_dim),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(embed_dim, embed_dim))\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, \n",
    "                                         num_heads=num_heads,\n",
    "                                         batch_first=True)\n",
    "\n",
    "    def forward(self, img, values):\n",
    "        img_ = self.img_mlp(img)\n",
    "        values_ = self.img_mlp(values)\n",
    "\n",
    "        attn_output, attn_output_weights = self.mha(img_, values_, values_)\n",
    "        \n",
    "        output = torch.bmm(attn_output_weights, values)\n",
    "\n",
    "        return output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "embed_dim = 256\n",
    "\n",
    "# Number of Heads!\n",
    "num_heads = 1\n",
    "\n",
    "# Batch Size!\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9361666",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = dataloader.DataLoader(dataset, shuffle=True, batch_size=batch_size, \n",
    "                                     num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "mha_model = AttentionTest(num_of_examples=num_of_examples, \n",
    "                          embed_dim=embed_dim, \n",
    "                          num_heads=num_heads).to(device)\n",
    "\n",
    "optimizer = optim.Adam(mha_model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_logger = []\n",
    "\n",
    "# Duplicate the data value tensor, one per batch element\n",
    "values_tensor = dataset_tensor.unsqueeze(0).expand(batch_size, num_of_examples, -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_model.train()\n",
    "for _ in trange(10, leave=False):\n",
    "    for data, _train_loader in tqdm(train_loader, leave=False):\n",
    "        q_img = data.reshape(data.shape[0], 1, -1).to(device)\n",
    "\n",
    "        attn_output, attn_output_weights = mha_model(q_img, values_tensor)\n",
    "\n",
    "        loss = (attn_output - q_img).pow(2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_logger.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_logger[100:])\n",
    "print(\"Minimum MSE loss %.4f\" % np.min(loss_logger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass of the model\n",
    "mha_model.eval()\n",
    "with torch.no_grad():\n",
    "    q_img = data.reshape(data.shape[0], 1, -1).to(device)\n",
    "    attn_output, attn_output_weights = mha_model(q_img, values_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given input use the attention map to find the \"closest\" value-data matches\n",
    "index = 10\n",
    "top10 = attn_output_weights[index, 0].argsort(descending=True)[:10]\n",
    "top10_data = values_tensor[index, top10].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52032b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (3, 3))\n",
    "out = torchvision.utils.make_grid(q_img[index].cpu().reshape(-1, 1, 28, 28), 8, \n",
    "                                  normalize=True, pad_value=0.5)\n",
    "\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the top 10 closest matches\n",
    "plt.figure(figsize = (10, 10))\n",
    "out = torchvision.utils.make_grid(top10_data.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(attn_output_weights[index, 0].cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the target and returned images\n",
    "target_img = q_img.reshape(batch_size, 1, 28, 28)\n",
    "indexed_img = attn_output.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "# Stack the images with the returned image on top\n",
    "img_pair = torch.cat((indexed_img, target_img), 2).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab48490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the pairs of images, the returned image on top, the target on bottom\n",
    "plt.figure(figsize = (10, 10))\n",
    "out = torchvision.utils.make_grid(img_pair, 8, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce75943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
