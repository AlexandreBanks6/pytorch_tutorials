{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Classifying text with LSTM!\n",
    "In this Notebook we'll first introduce techniques around processing text data. The field of Natural Language Processing (NLP) has many techniques that we have not yet looked into in this series, so we'll use this notebook to introduce them so that we can look at using text data in the future! \n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We'll be using Pytorch's text library called torchtext! \n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 20\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_len = 128\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "dataset_test = AG_NEWS(root=data_set_root, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer is the method by which we split the sentance into \"chunks\" or \"tokens\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# The vocab is all the unique tokens contained within our dataset\n",
    "# and provides each token with it's own integer index.\n",
    "\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentance to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(dataset_train),\n",
    "    min_freq=2, # Only include a token if it appears more than 2 times in the dataset\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'], # special case tokens\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set the <unk> \"unknown\" token as the default token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e22f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the vocab!\n",
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cab define \n",
    "text_tranform = T.Sequential(\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = lambda batch: [tokenizer(x) for x in batch]\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        return self.fc_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "\n",
    "# Create model\n",
    "lstm_classifier = LSTM(num_emb=len(vocab), output_size=4, \n",
    "                       num_layers=num_layers, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, nepochs, leave=False, desc=\"Epoch\")    \n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Test %.2f%%' % (train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    lstm_classifier.train()\n",
    "    steps = 0\n",
    "    for label, text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        bs = label.shape[0]\n",
    "        text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "        label = (label - 1).to(device)\n",
    "        \n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        pred, hidden, memory = lstm_classifier(text_tokens, hidden, memory)\n",
    "\n",
    "        loss = loss_fn(pred[:, -1, :], label)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        train_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "        steps += bs\n",
    "        \n",
    "    train_acc = (train_acc/steps).item()\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    lstm_classifier.eval()\n",
    "    steps = 0\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(data_loader_test, desc=\"Testing\", leave=False):\n",
    "            bs = label.shape[0]\n",
    "            text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "            label = (label - 1).to(device)\n",
    "\n",
    "            hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "            memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "            pred, hidden, memory = lstm_classifier(text_tokens, hidden, memory)\n",
    "\n",
    "            loss = loss_fn(pred[:, -1, :], label)\n",
    "            test_loss_logger.append(loss.item())\n",
    "\n",
    "            test_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "            steps += bs\n",
    "\n",
    "        test_acc = (test_acc/steps).item()\n",
    "        test_acc_logger.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b79069",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
