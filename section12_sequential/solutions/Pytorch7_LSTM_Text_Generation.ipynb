{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Text Generation, Many-to-Many with Text.\n",
    "If we treat a sentence as sequence of data-points, like a time series data source, we can perform \"next token\" prediction and create a model that tries to complete the text sequence based on an initial input. This is commonly refered to as \"text generation\".\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 20\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_len = 64\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "# dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "# data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb711b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNews(Dataset):\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/AG_NEWS/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Title\", \"Content\"])\n",
    "        \n",
    "        self.df.fillna('', inplace=True)\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4417ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = AGNews(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = AGNews(num_datapoints=data_set_root, test_train=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146ff12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Un-Comment to train sentence-piece model for tokenizer and vocab!\n",
    "\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/AG_NEWS/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7192529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the tokenizer!\n",
    "sp_model = load_sp_model(\"spm_ag_news.model\")\n",
    "\n",
    "# Returns a generator object\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# Iterate over tokens\n",
    "for token in tokenizer([\"i am creating\"]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031db2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "            \n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_ag_news.vocab\"), \n",
    "                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0),\n",
    ")\n",
    "\n",
    "gen_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = next(iter(data_loader_train))\n",
    "index = 0\n",
    "input_tokens = train_tranform(text)\n",
    "print(\"SENTENCE\")\n",
    "print(text[index])\n",
    "print()\n",
    "print(\"TOKENS\")\n",
    "print(vocab.lookup_tokens(input_tokens[index].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOKENS BACK TO SENTENCE\")\n",
    "\n",
    "pred_text = \"\".join(vocab.lookup_tokens(input_tokens[index].numpy()))\n",
    "pred_text.replace(\"▁\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, num_layers=1, emb_size=128, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_emb, emb_size)\n",
    "\n",
    "        self.mlp_emb = nn.Sequential(nn.Linear(emb_size, emb_size),\n",
    "                                     nn.LayerNorm(emb_size),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(emb_size, emb_size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.25)\n",
    "\n",
    "        self.mlp_out = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n",
    "                                     nn.LayerNorm(hidden_size//2),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Dropout(0.5),\n",
    "                                     nn.Linear(hidden_size//2, num_emb))\n",
    "        \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        input_embs = self.mlp_emb(input_embs)\n",
    "                \n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        return self.mlp_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# Create model\n",
    "lstm_generator = LSTM(num_emb=len(vocab), num_layers=num_layers, \n",
    "                      emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(lstm_generator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):    \n",
    "    lstm_generator.train()\n",
    "    steps = 0\n",
    "    for text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        text_tokens = train_tranform(list(text)).to(device)\n",
    "        bs = text_tokens.shape[0]\n",
    "        \n",
    "        # Randomly drop input tokens\n",
    "        input_text = td(text_tokens[:, 0:-1])\n",
    "        output_text = text_tokens[:, 1:]\n",
    "        \n",
    "        # Initialise the memory buffers\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        pred, hidden, memory = lstm_generator(input_text, hidden, memory)\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), output_text)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss_logger.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dde70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[100:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(entropy_logger[1000:])\n",
    "_ = plt.title(\"Distribution Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b610",
   "metadata": {},
   "source": [
    "## Generate some text!\n",
    "Lets use the fact that all of the articles have the title and content seperated by a : to get our model to generate some content based on a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples\n",
    "text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "temp = 0.9\n",
    "\n",
    "# We can either use an example from the test set or create our own article title!\n",
    "# init_prompt = [\"the next big thing from google :\"]\n",
    "init_prompt = [text[index].split(\":\")[0] + \":\"]\n",
    "\n",
    "\n",
    "input_tokens = gen_tranform(init_prompt).to(device)\n",
    "print(\"INITIAL PROMPT:\")\n",
    "print(init_prompt[0])\n",
    "\n",
    "print(\"\\nPROMPT TOKENS:\")\n",
    "print(input_tokens)\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c605133",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[index][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = []\n",
    "lstm_generator.eval()\n",
    "\n",
    "with torch.no_grad():    \n",
    "    hidden = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    memory = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    \n",
    "    for i in range(100):\n",
    "        data_pred, hidden, memory = lstm_generator(input_tokens, hidden, memory)\n",
    "#         We can take the token with the highest prob\n",
    "#         input_tokens = data_pred[:, -1].argmax().reshape(1, 1)\n",
    "        \n",
    "        # Or sample from the distribution of probs!\n",
    "        dist = Categorical(logits=data_pred[:, -1]/temp)\n",
    "        input_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        log_tokens.append(input_tokens.cpu())\n",
    "        if input_tokens.item() == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the raw tokens\n",
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the model's output with the initial title to get our article!\n",
    "init_prompt[0] + pred_text.replace(\"▁\", \" \").replace(\"<unk>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the probabilities\n",
    "_ = plt.plot(F.softmax(data_pred/temp, -1).cpu().numpy().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
